{
  "datasets": [
    {
      "task": "Question Identification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset contains about 31,000 questions labeled as comparative or not.\n\n",
      "link": "https://zenodo.org/records/7213397",
      "folder": "webis_comparative_questions_2022_qi",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task": "Question Identification",
      "name": "Webis Comparative Questions 2020",
      "description": "The Webis Comparative Web Search Questions 2020 (Webis-CompQuestions-20) corpus comprises 15,000 web questions collected from the public datasets MS Marco, Google Natural Questions and Quora. \n\nThe questions were manually annotated as comparative or not and with more fine-grained subclasses.\n\n",
      "link": "https://zenodo.org/records/4486547",
      "folder": "webis_comparative_questions_2020_qi",
      "paper": "Comparative Web Search Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3336191.3371848"
    },
    {
      "task" : "Question Identification",
      "name" : "Mintaka",
      "description" : "Mintaka is a complex, natural, and multilingual question answering (QA) dataset composed of 20,000 question-answer pairs elicited from MTurk workers and annotated with Wikidata question and answer entities.",
      "link":"https://github.com/amazon-science/mintaka/tree/main",
      "folder": "mintaka_qi",
      "paper": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
      "paper_link": "https://aclanthology.org/2022.coling-1.138/"
    },
    {
      "task" : "Question Identification",
      "name" : "Beloucif et al. (2022)",
      "description" : "In this dataset, we only \ntake into consideration binary comparatives; meaning that the input sentence should only compare two objects. An example of this would be: \"What's faster, Python or Matlab?\"\nWe consider open questions such as: \"Who's the best football player? to be none comparative\". \n\nFor this task, we asked Mechanical Turk annotators to label the questions as: comparative, not comparative or not a question. ",
      "link":"https://github.com/uhh-lt/Dataset-CompQA/tree/main/Classification%20Task",
      "folder": "beloucif_2022_qi",
      "paper": "Elvis vs. M. Jackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions",
      "paper_link": "https://aclanthology.org/2022.lrec-1.402/"
    },
    {
      "task": "Object and Aspect Identification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset contains about 3,500 comparative questions are labeled on the token level with comparison objects, aspects, predicates, or none.",
      "link": "https://zenodo.org/records/7213397",
      "folder": "webis_comparative_questions_2022_oai",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task" : "Object and Aspect Identification",
      "name" : "Beloucif et al. (2022)",
      "description" : "For this task annotators were asked to label each component of the comparison as follows: \n\"who\tis [better\tASPECT] manny\t[OBJ-1] or\t[ortiz\tOBJ-2]?\".",
      "link":"https://github.com/uhh-lt/Dataset-CompQA/tree/main/Labeling%20Task",
      "folder": "beloucif_2022_oai",
      "paper": "Elvis vs. M. Jackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions",
      "paper_link": "https://aclanthology.org/2022.lrec-1.402/"
    },
    {
      "task" : "Object and Aspect Identification",
      "name" : "Chekalina et al. (2021)",
      "description" : "",
      "link":"",
      "folder": "Chekalina_2021_oai",
      "paper": "Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language",
      "paper_link": "https://aclanthology.org/2021.eacl-demos.36/"
    },
    {
      "task": "Stance Classification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset for 950 questions contains text passages that potentially answer the questions are labeled with the stance: pro first comparison object, pro second, neutral, or no stance.",
      "link": "https://zenodo.org/records/7213397",
      "folder": "webis_comparative_questions_2022_sc",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task" : "Stance Classification",
      "name" : "CompSent-19",
      "description" : "The CompSent-19 dataset consists of 7,199 sentences annotated as BETTER, WORSE, or NONE, indicating whether the first object is better, worse, or not compared to the second object. The dataset spans three domains: computer science, brands, and random objects, minimizing domain-specific biases. Sentences were mined from the Common Crawl corpus using pairs of objects and comparative cue words to bias selection towards comparisons. Annotated via crowdsourcing, the dataset ensures high confidence in the labels, with 271 distinct object pairs and over 72% of sentences being non-comparative. In 70% of comparative sentences, the favored object is named first.",
      "link":"https://zenodo.org/records/3237552",
      "folder": "compsent_19_sc",
      "paper": "Categorizing Comparative Sentences",
      "paper_link": "https://aclanthology.org/W19-4516/"
    },
    {
      "task" : "Summary Generation",
      "name" : "Chekalina et al. (2021)",
      "description" : "",
      "link":"",
      "folder": "Chekalina_2021_sg",
      "paper": "Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language",
      "paper_link": "https://aclanthology.org/2021.eacl-demos.36/"
    }
  ]
}