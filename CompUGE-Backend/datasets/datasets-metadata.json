{
  "datasets": [
    {
      "task": "Question Identification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset contains about 31,000 questions labeled as comparative or not.\n\n",
      "link": "https://zenodo.org/records/7213397",
      "folder": "qi_webis_2022",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task": "Question Identification",
      "name": "Webis Comparative Questions 2020",
      "description": "The Webis Comparative Web Search Questions 2020 (Webis-CompQuestions-20) corpus comprises 15,000 web questions collected from the public datasets MS Marco, Google Natural Questions and Quora. \n\nThe questions were manually annotated as comparative or not and with more fine-grained subclasses.\n\n",
      "link": "https://zenodo.org/records/4486547",
      "folder": "qi_webis_2020",
      "paper": "Comparative Web Search Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3336191.3371848"
    },
    {
      "task": "Question Identification",
      "name": "CompUGE",
      "description": "This is a merged dataset, combining the Mintaka dataset with the Webis Comparative Questions 2020 and 2022 datasets. The Testset is based on Beloucif",
      "link": "",
      "folder": "qi_all",
      "paper": "",
      "paper_link": ""
    },
    {
      "task": "Object and Aspect Identification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset contains about 3,500 comparative questions are labeled on the token level with comparison objects, aspects, predicates, or none. \n \n Our version of this dataset has it's labels converted to 0,1,2,3,4 indicating NONE, B-Object, I-Object, B-Aspect, I-Aspect respectively. All predicates are labeled as Aspects.",
      "link": "https://zenodo.org/records/7213397",
      "folder": "oai_webis_2022",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task": "Object and Aspect Identification",
      "name": "Beloucif et al. (2022)",
      "description": "For this task annotators were asked to label each component of the comparison as follows: \n\"who\tis [better\tASPECT] manny\t[OBJ-1] or\t[ortiz\tOBJ-2]?\". \n \nOur version of this dataset has it's labels converted to 0,1,2,3,4 indicating NONE, B-Object, I-Object, B-Aspect, I-Aspect respectively. We removed all questions that made use of the SHARED label.",
      "link": "https://github.com/uhh-lt/Dataset-CompQA/tree/main/Labeling%20Task",
      "folder": "oai_beloucif",
      "paper": "Elvis vs. M. Jackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions",
      "paper_link": "https://aclanthology.org/2022.lrec-1.402/"
    },
    {
      "task": "Object and Aspect Identification",
      "name": "CompUGE",
      "description": "This is a merged dataset, combining the Webis Comparative Questions 2022 dataset with the Beloucif et al. (2022) dataset. After processing the data.",
      "link": "",
      "folder": "oai_webis_beloucif_merged",
      "paper": "",
      "paper_link": ""
    },
    {
      "task": "Stance Classification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset for 950 questions contains text passages that potentially answer the questions are labeled with the stance: pro first comparison object, pro second, neutral, or no stance.",
      "link": "https://zenodo.org/records/7213397",
      "folder": "sc_webis_2022",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task": "Stance Classification",
      "name": "CompSent-19",
      "description": "The CompSent-19 dataset consists of 7,199 sentences annotated as BETTER, WORSE, or NONE, indicating whether the first object is better, worse, or not compared to the second object. The dataset spans three domains: computer science, brands, and random objects, minimizing domain-specific biases. Sentences were mined from the Common Crawl corpus using pairs of objects and comparative cue words to bias selection towards comparisons. Annotated via crowdsourcing, the dataset ensures high confidence in the labels, with 271 distinct object pairs and over 72% of sentences being non-comparative. In 70% of comparative sentences, the favored object is named first.",
      "link": "https://zenodo.org/records/3237552",
      "folder": "sc_compsent_19",
      "paper": "Categorizing Comparative Sentences",
      "paper_link": "https://aclanthology.org/W19-4516/"
    },
    {
      "task": "Stance Classification",
      "name": "CompUGE",
      "description": "This is a merged dataset, combining the CompSent-19 dataset with the Webis Comparative Questions 2022 dataset. After processing the data.",
      "link": "",
      "folder": "sc_all",
      "paper": "",
      "paper_link": ""
    },
    {
      "task": "Summary Generation",
      "name": "Chekalina et al. (2021)",
      "description": "",
      "link": "",
      "folder": "sg_chekalina_2021",
      "paper": "Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language",
      "paper_link": "https://aclanthology.org/2021.eacl-demos.36/"
    }
  ]
}