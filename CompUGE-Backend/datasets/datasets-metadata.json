{
  "datasets": [
    {
      "task": "Question Identification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset contains about 31,000 questions labeled as comparative or not.\n\n",
      "link": "https://zenodo.org/records/7213397",
      "folder": "qi_webis_2022",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task": "Question Identification",
      "name": "Webis Comparative Questions 2020",
      "description": "The Webis Comparative Web Search Questions 2020 (Webis-CompQuestions-20) corpus comprises 15,000 web questions collected from the public datasets MS Marco, Google Natural Questions and Quora. \n\nThe questions were manually annotated as comparative or not and with more fine-grained subclasses.\n\n",
      "link": "https://zenodo.org/records/4486547",
      "folder": "qi_webis_2020",
      "paper": "Comparative Web Search Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3336191.3371848"
    },
    {
      "task": "Question Identification",
      "name": "Mintaka",
      "description": "Mintaka is a complex, natural, and multilingual question answering (QA) dataset composed of 20,000 question-answer pairs elicited from MTurk workers and annotated with Wikidata question and answer entities.",
      "link": "https://github.com/amazon-science/mintaka/tree/main",
      "folder": "qi_mintaka",
      "paper": "Mintaka: A Complex, Natural, and Multilingual Dataset for End-to-End Question Answering",
      "paper_link": "https://aclanthology.org/2022.coling-1.138/"
    },
    {
      "task": "Question Identification",
      "name": "Beloucif et al. (2022)",
      "description": "In this dataset, we only \ntake into consideration binary comparatives; meaning that the input sentence should only compare two objects. An example of this would be: \"What's faster, Python or Matlab?\"\nWe consider open questions such as: \"Who's the best football player? to be none comparative\". \n\nFor this task, we asked Mechanical Turk annotators to label the questions as: comparative, not comparative or not a question. ",
      "link": "https://github.com/uhh-lt/Dataset-CompQA/tree/main/Classification%20Task",
      "folder": "qi_beloucif",
      "paper": "Elvis vs. M. Jackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions",
      "paper_link": "https://aclanthology.org/2022.lrec-1.402/"
    },
    {
      "task": "Question Identification",
      "name": "Mintaka, Webis 2020, Webis 2022",
      "description": "This is a merged dataset, combining the Mintaka dataset with the Webis Comparative Questions 2020 and 2022 datasets.",
      "link": "",
      "folder": "qi_all",
      "paper": "",
      "paper_link": ""
    },
    {
      "task": "Object and Aspect Identification",
      "name": "Webis Comparative Questions 2022",
      "description": "Webis Comparative Questions 2022 dataset contains about 3,500 comparative questions are labeled on the token level with comparison objects, aspects, predicates, or none. \n \n Our version of this dataset has it's labels converted to 0,1,2,3,4 indicating NONE, B-Object, I-Object, B-Aspect, I-Aspect respectively. All predicates are labeled as Aspects.",
      "link": "https://zenodo.org/records/7213397",
      "folder": "oai_webis_2022",
      "paper": "Towards Understanding and Answering Comparative Questions",
      "paper_link": "https://dl.acm.org/doi/10.1145/3488560.3498534"
    },
    {
      "task": "Object and Aspect Identification",
      "name": "Beloucif et al. (2022)",
      "description": "For this task annotators were asked to label each component of the comparison as follows: \n\"who\tis [better\tASPECT] manny\t[OBJ-1] or\t[ortiz\tOBJ-2]?\". \n \nOur version of this dataset has it's labels converted to 0,1,2,3,4 indicating NONE, B-Object, I-Object, B-Aspect, I-Aspect respectively. We removed all questions that made use of the SHARED label.",
      "link": "https://github.com/uhh-lt/Dataset-CompQA/tree/main/Labeling%20Task",
      "folder": "oai_beloucif",
      "paper": "Elvis vs. M. Jackson: Who has More Albums? Classification and Identification of Elements in Comparative Questions",
      "paper_link": "https://aclanthology.org/2022.lrec-1.402/"
    },
    {
      "task": "Object and Aspect Identification",
      "name": "Webis, Beloucif",
      "description": "This is a merged dataset, combining the Webis Comparative Questions 2022 dataset with the Beloucif et al. (2022) dataset.",
      "link": "",
      "folder": "oai_webis_beloucif_merged",
      "paper": "",
      "paper_link": ""
    },
    {
      "task": "Stance Classification",
      "name": "CompSent-19",
      "description": "The CompSent-19 dataset consists of 7,199 sentences annotated as BETTER, WORSE, or NONE, indicating whether the first object is better, worse, or not compared to the second object. The dataset spans three domains: computer science, brands, and random objects, minimizing domain-specific biases. Sentences were mined from the Common Crawl corpus using pairs of objects and comparative cue words to bias selection towards comparisons. Annotated via crowdsourcing, the dataset ensures high confidence in the labels, with 271 distinct object pairs and over 72% of sentences being non-comparative. In 70% of comparative sentences, the favored object is named first. \n \n Our version of this dataset has it's labels converted to 0,1,2 indicating NONE, BETTER, WORSE.",
      "link": "https://zenodo.org/records/3237552",
      "folder": "sc_compsent_19",
      "paper": "Categorizing Comparative Sentences",
      "paper_link": "https://aclanthology.org/W19-4516/"
    },
    {
      "task": "Summary Generation",
      "name": "Chekalina et al. (2021)",
      "description": "",
      "link": "",
      "folder": "sg_chekalina_2021",
      "paper": "Which is Better for Deep Learning: Python or MATLAB? Answering Comparative Questions in Natural Language",
      "paper_link": "https://aclanthology.org/2021.eacl-demos.36/"
    }
  ]
}